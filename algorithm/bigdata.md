# 大数据量场景题目

## ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) `哈希函数统计词频，在40亿数据中统计出top10频率最高的字符串`

哈希函数可以将任意长度出入转化为固定长度输出（通常输出空间远小于输入），相同输入输出值一定相同。且输出结果是均匀分布。

使用数据流读取文件，对每个字符串进行哈希计算然后对N（本题100）求模，按照计算结果将40亿数据分流到100个文件中。分别计算每个文件词频top10，可以放到内存排序。
或者在外存中使用归并，快速查找（类似快排只要第K个时候使用），小顶堆算法计算出结果。

计算文件大小： 一个int是4字节 每个数字，40亿就是 160亿字节 = 16百万KB = 16千MB = 16GB； 文件数量N取决于要排序的文件大小和内存大小。
10亿KB = 1百万MB = 1000GB = 1TB

## ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) `布隆过滤器，查找相同字符串`

要允许有失误率（有过记录的一定能发现，但是没有记录的有可能错判），黑名单过滤系统应用。
给一个数据判断他是否在名单中（或者是否在这个文件系统中）使用布隆过滤器。
三个公式可以定量计算，还有需要定性分析。

m个位的bit的位图。使用K个哈希函数，一个文件K个结果所在位都涂黑。然后新来的数据做K个哈希结果如果都是黑的就判断在黑名单里面。
n个样本（样本个数），S字节（无关）。随着m增加，失误率p是下降的。m向上取整。p是失误率。k增加，失误率p先降低然后在增加。存在k有p极值。

详细的公式和计算逻辑
![](bloomfilter.jpg)

## ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) `哈希一致性，分布式负载均衡`

管理分布式的负载均衡，创建一个N为2的32次幂的哈希环，对数据进行哈希然后对N求模，将数据均匀的分布在哈希环上。每个服务器根据其性能创建不同数量的字符串。
每个字符串hash在环上对应的点就是一个虚拟节点。维护一张实体服务器和虚拟节点的对应关系就可以很好的管理负载均衡了。因为哈希的性质当数据量很大时候分布均匀。

## ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) `位图法查找43亿数字中没出现的，43亿是2的32次方个数字。`

使用位图，43亿位bit换算是5亿多字节，500M搞定，数字出现就在这位标1没出现标0。最终打印标0的位数。

统计哪些出现了2次，2个bit来标记一个数，00没出现，01出现一次，10出现2次，11出现两次以上。1G空间可以算43亿数字。

空间限制特别小10M（3KB），只找一个没出现的。3KB处以4字节可以创建一个512长度的数组（取2的N次幂），然后将43亿数分成512个区间，统计区间词频。
每个区间词频放入数组，当数组中某一个区间小于应该出现的次数43亿除512，重复这个过程，最后能找到没出现的数。遍历文件次数log（512） 43亿 次数

如果用有限几个变量就使用二分法，每次取中点统计两边次数，小于应该的次数的区间继续进行二分。log2 N次的遍历次数。

分区间统计分段思想找中位数，512数组中从左向右加，看第几个结果刚好包括中位数，然后在这个区间继续分统计，一直到最后找到中位数。

二维大根堆，左右文件内部构成大根堆，然后每个大根堆的对顶构成一个大根堆。每取出一个对顶，文件内选出一个新的对顶，然后二维堆重新排序。


## ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) `1000T数据给10G内存排序`














